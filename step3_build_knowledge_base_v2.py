# è®© RAG èƒ½çœ‹åˆ° hea_element_stats.csv çš„å†…å®¹ï¼Œé€šè¿‡load_element_statsåŠ è½½"hea_element_stats.csv"ï¼Œç»™è‡ªå·±å‘è¡¨çš„ä¸¤ç¯‡æ–‡ç« æ‰“æ ‡ç­¾
import os
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, Docx2txtLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
load_dotenv()

MY_PAPERS = {
    "my-paper1-pan.pdf",
    "my-paper2-shan.pdf",
}

# ========= 1. è·¯å¾„å®šä¹‰ï¼ˆå…¨éƒ¨å°è£…æˆ Pathï¼Œä¸è¦ç”¨ strï¼‰ =========
DATA_DIR  = Path(r"E:\AMsystem\Project\QCH-RAG\data")
PAPER_DIR = Path(r"E:\AMsystem\Project\QCH-RAG\data\papers\pdf")
WORD_DIR  = Path(r"E:\AMsystem\Project\QCH-RAG\data\papers\word")
DB_DIR    = Path(r"E:\AMsystem\Project\QCH-RAG\db\8")

print("DATA_DIR =", DATA_DIR, type(DATA_DIR))
print("PAPER_DIR =", PAPER_DIR, type(PAPER_DIR))
print("WORD_DIR  =", WORD_DIR, type(WORD_DIR))
print("DB_DIR =", DB_DIR, type(DB_DIR))


# ========= 2. åŠ è½½ PDF æ–‡çŒ® =========
def load_pdfs():
    docs = []

    if not PAPER_DIR.exists():
        print(f"âŒ PDF ç›®å½•ä¸å­˜åœ¨ï¼š{PAPER_DIR}")
        return docs

    pdf_files = list(PAPER_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"âš ï¸ åœ¨ {PAPER_DIR} ä¸‹æ²¡æœ‰æ‰¾åˆ° pdf æ–‡ä»¶ï¼Œè¯·å…ˆæ”¾å…¥ PDF æ–‡çŒ®ã€‚")
        return docs

    for pdf_path in pdf_files:
        print(f"åŠ è½½ PDFï¼š{pdf_path.name}")
        loader = PyPDFLoader(str(pdf_path))  # è¿™é‡Œæ‰è½¬ä¸ºå­—ç¬¦ä¸²
        pdf_docs = loader.load()

        # ğŸ”¥ æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ˜¯ä¸æ˜¯â€œæˆ‘çš„è®ºæ–‡â€
        is_my_paper = pdf_path.name in MY_PAPERS

        for d in pdf_docs:
            d.metadata["source_type"] = "pdf"
            d.metadata["source_file"] = pdf_path.name
            d.metadata["is_my_paper"] = is_my_paper  # å…³é”®æ ‡è®°

        docs.extend(pdf_docs)

    print(f"å…±ä» PDF åŠ è½½åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—ã€‚")
    return docs


# ========= 3. åŠ è½½ CSV æ•°æ® =========
def load_csv():
    csv_path = DATA_DIR / "her_hea_literature_clean.csv"

    if not csv_path.exists():
        print(f"âŒ æœªæ‰¾åˆ° CSV æ–‡ä»¶ï¼š{csv_path}")
        return []

    print(f"åŠ è½½ CSVï¼š{csv_path.name}")
    loader = CSVLoader(
        file_path=str(csv_path),
        encoding="utf-8-sig"
    )

    csv_docs = loader.load()

    for d in csv_docs:
        d.metadata["source_type"] = "csv"
        d.metadata["source_file"] = csv_path.name

    print(f"ä» CSV è¯»å–åˆ° {len(csv_docs)} æ¡æ–‡çŒ®æ•°æ®ã€‚")
    return csv_docs


# ========= 4. åŠ è½½ Wordï¼ˆ.docxï¼‰ä»ªå™¨/è®¾å¤‡è¯´æ˜ =========
def load_word_docs():
    docs = []

    if not WORD_DIR.exists():
        print(f"âš ï¸ Word ç›®å½•ä¸å­˜åœ¨ï¼š{WORD_DIR}ï¼ˆå¦‚æœæœ‰ä»ªå™¨è¯´æ˜ Wordï¼Œè¯·å…ˆåˆ›å»ºè¯¥ç›®å½•å¹¶æ”¾å…¥ .docx æ–‡ä»¶ï¼‰")
        return docs

    docx_files = list(WORD_DIR.glob("*.docx"))
    if not docx_files:
        print(f"âš ï¸ åœ¨ {WORD_DIR} ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .docx æ–‡ä»¶ã€‚")
        return docs

    for docx_path in docx_files:
        print(f"åŠ è½½ Word æ–‡æ¡£ï¼š{docx_path.name}")
        loader = Docx2txtLoader(str(docx_path))
        word_docs = loader.load()

        for d in word_docs:
            d.metadata["source_type"] = "word"
            d.metadata["source_file"] = docx_path.name

        docs.extend(word_docs)

    print(f"âœ… å…±ä» Word æ–‡æ¡£åŠ è½½åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—ã€‚")
    return docs


def load_element_stats():
    """åŠ è½½ hea_element_stats.csvï¼Œä¾› RAG ä½¿ç”¨"""
    stats_path = DATA_DIR / "hea_element_stats.csv"
    if not stats_path.exists():
        print(f"âš ï¸ æœªæ‰¾åˆ°å…ƒç´ ç»Ÿè®¡è¡¨ï¼š{stats_path}")
        return []

    print(f"åŠ è½½å…ƒç´ ç»Ÿè®¡è¡¨ï¼š{stats_path.name}")
    loader = CSVLoader(
        file_path=str(stats_path),
        encoding="utf-8-sig",
    )
    docs = loader.load()

    for d in docs:
        d.metadata["source_type"] = "element_stats"
        d.metadata["source_file"] = stats_path.name

    print(f"âœ… ä»å…ƒç´ ç»Ÿè®¡è¡¨è¯»å–åˆ° {len(docs)} æ¡è®°å½•ã€‚")
    return docs


# ========= 5. æ–‡æœ¬åˆ‡å‰² =========
def split_docs(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,  # 1000
        chunk_overlap=200,  # 200
        separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""],
    )
    docs = splitter.split_documents(documents)
    print(f"åˆ‡åˆ†åå¾—åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—ã€‚")
    return docs


# ========= 6. æ„å»ºå‘é‡åº“ =========
def build_vector_store(docs):
    # if "OPENAI_API_KEY" not in os.environ:
    #     print("âŒ æœªæ£€æµ‹åˆ° OPENAI_API_KEYï¼Œè¯·å…ˆè®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ã€‚")
    #     return
    #
    # embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    embedding_api_key = os.getenv("OPENAI_EMBEDDING_API_KEY")
    embedding_base_url = os.getenv("OPENAI_EMBEDDING_BASE_URL")

    if not embedding_api_key or not embedding_base_url:
        print("âŒ æœªæ£€æµ‹åˆ° OPENAI_EMBEDDING_API_KEY æˆ– OPENAI_EMBEDDING_BASE_URLï¼Œè¯·åœ¨ .env æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­é…ç½®ã€‚")
        return

    # âš ï¸ è¿™é‡Œçš„ model åç§°è¦ä¸ä½ åœ¨å¯¹åº”å¹³å°å¼€é€šçš„ embedding æ¨¡å‹ä¸€è‡´
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",  # å¦‚æœä½ é‚£è¾¹æ˜¯åˆ«çš„åå­—ï¼Œå°±æ”¹æˆå¯¹åº”çš„
        api_key=embedding_api_key,
        base_url=embedding_base_url,
    )

    if not DB_DIR.exists():
        DB_DIR.mkdir(parents=True, exist_ok=True)

    print(f"å¼€å§‹æ„å»ºå‘é‡åº“åˆ°ï¼š{DB_DIR}")
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=str(DB_DIR),
    )
    vectorstore.persist()
    print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")


# ========= 7. ä¸»ç¨‹åº =========
def main():
    pdf_docs = load_pdfs()
    csv_docs = load_csv()
    word_docs = load_word_docs()
    stats_docs = load_element_stats()  # âœ… å…ƒç´ ç»Ÿè®¡è¡¨

    all_docs = pdf_docs + csv_docs + word_docs + stats_docs
    if not all_docs:
        print("âš ï¸ æ²¡æœ‰ä»»ä½•æ–‡æ¡£ç”¨äºæ„å»º RAG çŸ¥è¯†åº“ï¼Œè¯·æ£€æŸ¥ PDF / CSV / Wordã€‚")
        return

    split = split_docs(all_docs)
    build_vector_store(split)


if __name__ == "__main__":
    main()